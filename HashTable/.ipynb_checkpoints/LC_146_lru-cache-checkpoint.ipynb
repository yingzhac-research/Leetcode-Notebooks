{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 146. LRU Cache"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Topic Alignment\n- **Role Relevance**: Designing LRU caches underpins feature store hot-cache design and intermediate result memoization in ML systems.\n- **Scenario**: Helps manage scarce GPU inference cache space by evicting least recently used embeddings."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Metadata Summary\n- Source: [LeetCode - LRU Cache](https://leetcode.com/problems/lru-cache/)\n- Tags: `Hash Table`, `Linked List`, `Design`\n- Difficulty: Medium\n- Recommended Priority: High"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Problem Statement\nDesign a data structure that follows the constraints of a Least Recently Used (LRU) cache.\n\nImplement the `LRUCache` class:\n- `LRUCache(int capacity)` initializes the cache with positive size capacity.\n- `int get(int key)` returns the value of the key if it exists, otherwise returns `-1`.\n- `void put(int key, int value)` updates or inserts the key-value pair. When the cache reaches capacity, it should invalidate the least recently used item before inserting a new one.\n\nBoth operations must run in `O(1)` average time."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Constraints\n- `1 <= capacity <= 3000`\n- `0 <= key <= 10^4`\n- `0 <= value <= 10^5`\n- At most `2 * 10^5` calls will be made to `get` and `put`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Progressive Hints\n- Hint 1: Use a hash map to reach nodes in constant time by key.\n- Hint 2: Maintain usage order with a doubly linked list so you can move nodes to the front on access.\n- Hint 3: Keep dummy head and tail nodes to simplify insertions and deletions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Solution Overview\nCombine a hash map with a doubly linked list: the map stores references to nodes for `O(1)` access, while the list tracks usage order so the least recently used node sits at the tail ready for eviction."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Detailed Explanation\n1. Create a doubly linked list with dummy head and tail nodes to represent most/least recently used ends.\n2. Maintain a dictionary mapping keys to their corresponding nodes.\n3. On `get`, look up the node; if present, move it to the front (most recent) and return its value.\n4. On `put`, if the key exists, update the value and move the node to the front.\n5. If the key is new and capacity is reached, evict the node at the tail, removing it from both the list and dictionary.\n6. Insert the new node at the front and update the dictionary."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Complexity Trade-off Table\n| Approach | Time Complexity | Space Complexity | Notes |\n| --- | --- | --- | --- |\n| Hash map + doubly linked list | O(1) | O(capacity) | Industry-standard LRU design. |\n| OrderedDict (Python) | O(1) | O(capacity) | Simpler but relies on language-specific utilities. |"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class LRUCache:\n    \"\"\"Least Recently Used cache implemented with a hash map and doubly linked list.\"\"\"\n\n    class Node:\n        __slots__ = (\"key\", \"value\", \"prev\", \"next\")\n\n        def __init__(self, key: int, value: int) -> None:\n            self.key = key\n            self.value = value\n            self.prev = None\n            self.next = None\n\n    def __init__(self, capacity: int):\n        self.capacity = capacity\n        self.cache = {}  # Maps keys to nodes for O(1) access.\n        self.head = self.Node(0, 0)  # Dummy head.\n        self.tail = self.Node(0, 0)  # Dummy tail.\n        self.head.next = self.tail\n        self.tail.prev = self.head\n\n    def _remove(self, node: \"LRUCache.Node\") -> None:\n        prev_node = node.prev\n        next_node = node.next\n        prev_node.next = next_node\n        next_node.prev = prev_node\n\n    def _add_to_front(self, node: \"LRUCache.Node\") -> None:\n        node.prev = self.head\n        node.next = self.head.next\n        self.head.next.prev = node\n        self.head.next = node\n\n    def get(self, key: int) -> int:\n        node = self.cache.get(key)\n        if not node:\n            return -1\n        self._remove(node)  # Move accessed node to front.\n        self._add_to_front(node)\n        return node.value\n\n    def put(self, key: int, value: int) -> None:\n        node = self.cache.get(key)\n        if node:\n            node.value = value  # Update value.\n            self._remove(node)\n            self._add_to_front(node)\n            return\n        if len(self.cache) == self.capacity:\n            lru_node = self.tail.prev  # Node to evict.\n            self._remove(lru_node)\n            del self.cache[lru_node.key]\n        new_node = self.Node(key, value)\n        self.cache[key] = new_node\n        self._add_to_front(new_node)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Complexity Analysis\n- Time Complexity: `O(1)` per operation for `get` and `put`.\n- Space Complexity: `O(capacity)` for the dictionary and linked list nodes.\n- Bottleneck: Pointer updates when moving nodes; still constant time."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Edge Cases & Pitfalls\n- Capacity can be as low as 1; ensure eviction logic works for this case.\n- Avoid memory leaks by fully unlinking nodes during eviction.\n- Ensure that `put` on existing keys updates the value rather than duplicating nodes."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Follow-up Variants\n- Extend to LFU (least frequently used) cache by combining frequency counters with hash maps.\n- Add time-to-live semantics for expiring stale entries.\n- Make the cache thread-safe for concurrent inference workloads."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Takeaways\n- Hash maps combined with auxiliary structures unlock O(1) operations for stateful services.\n- Dummy sentinel nodes simplify linked list operations.\n- Real-world cache design often layers additional policies atop LRU."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Similar Problems\n| Problem ID | Problem Title | Technique |\n| --- | --- | --- |\n| 460 | LFU Cache | Hash map + frequency list |\n| 432 | All O(1) Data Structure | Hash map + double linked list |\n| 707 | Design Linked List | Doubly linked list manipulation |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}