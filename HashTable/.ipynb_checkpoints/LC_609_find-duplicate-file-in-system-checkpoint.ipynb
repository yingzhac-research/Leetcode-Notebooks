{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 609. Find Duplicate File in System\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topic Alignment\n",
        "- MLE Connection: Detecting duplicate artifacts mirrors deduplicating feature files or experiment outputs.\n",
        "- Hash Table Role: Use file content as the key and append full paths to group duplicates.\n",
        "- Interview Angle: Parses structured strings and applies inverted indexing efficiently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metadata Summary\n",
        "- Source: https://leetcode.com/problems/find-duplicate-file-in-system/\n",
        "- Tags: Hash Table, String, File System\n",
        "- Difficulty: Medium\n",
        "- Recommended Review Priority: Medium\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Statement\n",
        "You are given a list of directory info strings. Each string has the form \"root/dir file1(content1) file2(content2) ...\". Construct a list of groups where each group contains the full paths of files that share identical content. Paths should be constructed as \"directory_path/file_name\". Only groups with more than one path should be returned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constraints\n",
        "- 1 <= paths.length <= 200.\n",
        "- 1 <= paths[i].length <= 3000.\n",
        "- 1 <= total number of files <= 20,000.\n",
        "- Directory and file names consist of lowercase letters, digits, and '/'.\n",
        "- File content is non-empty and enclosed in parentheses without nested parentheses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Progressive Hints\n",
        "- Hint 1: Parse each directory string into a root path plus name-content pairs.\n",
        "- Hint 2: Use file content as the hash key and append the full path to that list.\n",
        "- Hint 3: After processing all entries, return only the lists where the length is at least two.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution Overview\n",
        "Parse each directory description, extracting file names and their contents. Build a dictionary keyed by content with values being the list of full file paths. Filter to groups of size greater than one and return them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Explanation\n",
        "1. For each input string, split on spaces to separate the directory root from the file entries.\n",
        "2. For each file entry, locate the '(' to split the file name from the content, and remove the trailing ')'.\n",
        "3. Build the full path using `root + '/' + file_name` and append it to `content_map[content]`.\n",
        "4. After ingesting all data, iterate over the dictionary values and select only those lists with at least two entries.\n",
        "\n",
        "This inverted index keyed by content groups duplicates in linear time over the total characters parsed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complexity Trade-off Table\n",
        "| Approach | Time Complexity | Space Complexity |\n",
        "| --- | --- | --- |\n",
        "| Compare every file pair | O(m^2 * L) | O(1) |\n",
        "| Hash content to paths | O(m * L) | O(m * L) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reference Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class Solution:\n",
        "    def findDuplicate(self, paths: List[str]) -> List[List[str]]:\n",
        "        content_map: defaultdict[str, List[str]] = defaultdict(list)\n",
        "\n",
        "        for entry in paths:\n",
        "            parts = entry.split()\n",
        "            root = parts[0]\n",
        "            for file_info in parts[1:]:\n",
        "                name, content = self._parse_file(file_info)\n",
        "                full_path = f\"{root}/{name}\"\n",
        "                content_map[content].append(full_path)\n",
        "\n",
        "        return [group for group in content_map.values() if len(group) > 1]\n",
        "\n",
        "    def _parse_file(self, file_info: str) -> tuple[str, str]:\n",
        "        open_idx = file_info.index('(')\n",
        "        name = file_info[:open_idx]\n",
        "        content = file_info[open_idx + 1:-1]\n",
        "        return name, content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complexity Analysis\n",
        "- Time Complexity: O(m * L) where m is the number of files and L is the average length of each file description.\n",
        "- Space Complexity: O(m * L) to store content strings and associated paths.\n",
        "- Bottlenecks: Parsing long directory strings dominates but remains linear overall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Edge Cases & Pitfalls\n",
        "- No group should be returned if a content appears only once.\n",
        "- Some directories may contribute zero duplicates; they should not appear in the output.\n",
        "- Watch for very long content strings; avoid redundant slicing beyond necessary parsing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Follow-up Variants\n",
        "- Detect duplicates across machines by hashing file contents rather than storing the entire content.\n",
        "- Handle empty files or binary data encoded in hexadecimal.\n",
        "- Return a representative file for each group to facilitate deduplication.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways\n",
        "- Inverted indexes are a natural fit for grouping by shared attributes.\n",
        "- Parsing structured strings carefully avoids off-by-one errors and overhead.\n",
        "- Filtering after grouping keeps the main logic simple and efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Similar Problems\n",
        "| Problem ID | Problem Title | Technique |\n",
        "| --- | --- | --- |\n",
        "| 49 | Group Anagrams | Signature grouping |\n",
        "| 652 | Find Duplicate Subtrees | Structural hashing |\n",
        "| 187 | Repeated DNA Sequences | Hash by content |\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}